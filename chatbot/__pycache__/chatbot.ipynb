{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361d02ce-138f-43b0-9397-85d41e4e40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path:str, tokenizer):\n",
    "        self.data = json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for i in self.data:\n",
    "            for j in i['dialog']:\n",
    "                self.X.append(j['text'])\n",
    "\n",
    "        for idx, i in enumerate(self.X):\n",
    "            try:\n",
    "                self.X[idx] = \"<startofstring> \"+i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        self.X = self.X[:5000]\n",
    "        \n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845a0ef-ffff-405f-96dc-79d2ffc90264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9867e54cc35409b82319bfb98056c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b4e12bedd84f6fbc419a648dcfb726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e6cf5a0b4f428c95fa9c259c188c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef668509fccd433aa7ee4efd0f1edc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bc69e2e3874439a48bf5f99dcbbe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5241f1f25d406d9434f60dac2bff42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50261. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> I love iphone! i just bought new iphone! <bot>: Thats good for you, i'm not very into new tech <endofstring>\n",
      "training .... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "  8%|██████▉                                                                            | 1/12 [00:28<05:15, 28.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: I am a good??? <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      " 17%|█████████████▊                                                                     | 2/12 [00:52<04:19, 25.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi? <bot>: Hi? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|████████████████████▊                                                              | 3/12 [01:16<03:45, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: i am a huge gamer. i am a huge gamer. <endofstring> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███████████████████████████▋                                                       | 4/12 [01:40<03:15, 24.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|██████████████████████████████████▌                                                | 5/12 [02:04<02:52, 24.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: hello <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████████████████████████████████████████▌                                         | 6/12 [02:31<02:30, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi, how are doing? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|████████████████████████████████████████████████▍                                  | 7/12 [02:57<02:06, 25.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: i am a huge gamer <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 8/12 [03:23<01:42, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 9/12 [03:47<01:15, 25.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: i am a huge gamer <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 10/12 [04:12<00:50, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: hello how are you? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 11/12 [04:36<00:24, 24.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [05:00<00:00, 25.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello how are you <bot>: Hi, how are doing? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "infer from model : \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hey <bot>: Hi, how are doing? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hi <bot>: Hi, how are doing? <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " i am not feeling good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> i am not feeling good <bot>: i am not feeling really. i am just sitting at home right\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> how are you <bot>: i am a huge gamer, my mom is a very good person. <endofstring>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what are you doing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> what are you doing <bot>: i am not sure what that is. i am a very experienced person\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how are you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> how are you <bot>: I am doing great, how are you? <endofstring> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " i am feeling sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> i am feeling sad <bot>: i am a huge gamer, my mom is a very good person.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " i am not feeling good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> i am not feeling good <bot>: i am not feeling really. i am just sitting here. <endofstring>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from ChatData import ChatData\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "def train(chatData, model, optim):\n",
    "\n",
    "    epochs = 12\n",
    "\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        for X, a in chatData:\n",
    "            X = X.to(device)\n",
    "            a = a.to(device)\n",
    "            optim.zero_grad()\n",
    "            loss = model(X, attention_mask=a, labels=X).loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        torch.save(model.state_dict(), \"model_state.pt\")\n",
    "        print(infer(\"hello how are you\"))\n",
    "\n",
    "def infer(inp):\n",
    "    inp = \"<startofstring> \"+inp+\" <bot>: \"\n",
    "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    output = model.generate(X, attention_mask=a )\n",
    "    output = tokenizer.decode(output[0])\n",
    "    return output\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                \"bos_token\": \"<startofstring>\",\n",
    "                                \"eos_token\": \"<endofstring>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# print(tokenizer.decode(model.generate(**tokenizer(\"hey i was good at basketball but \",\n",
    "#                          return_tensors=\"pt\"))[0]))\n",
    "\n",
    "chatData = ChatData(\"chat_data.json\", tokenizer)\n",
    "chatData =  DataLoader(chatData, batch_size=64)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"training .... \")\n",
    "train(chatData, model, optim)\n",
    "\n",
    "print(\"infer from model : \")\n",
    "while True:\n",
    "  inp = input()\n",
    "  print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783a991-b5b1-4bdc-9c63-27b74c65efab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
